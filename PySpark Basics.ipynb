{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark Connection:\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark_Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print PySpark and Python versions\n",
    "import sys\n",
    "print('Python version: '+sys.version)\n",
    "print('Spark version: '+spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from Files (.csv files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data from FILE (with infer_schema = \"false\"):\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "file_location = \"movie_data_part1.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \"|\"\n",
    "\n",
    "df = spark.read.format(file_type)\\\n",
    ".option(\"InferSchema\", infer_schema)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(file_location)\n",
    "\n",
    "end = time.time()\n",
    "time_taken = (end - start)\n",
    "print(f\"Total time taken {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Metadata\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data from FILE (with infer_schema = \"true\"):\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "file_location = \"movie_data_part1.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \"|\"\n",
    "\n",
    "df = spark.read.format(file_type)\\\n",
    ".option(\"InferSchema\", infer_schema)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(file_location)\n",
    "\n",
    "end = time.time()\n",
    "time_taken = (end - start)\n",
    "print(f\"Total time taken {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from Files (.json files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['id','timestamp'].show(4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print Metadata\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of printing the data types:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the total number of records:\n",
    "df.count()\n",
    "print(f\"Total number of records in the dataset: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the dataframe:\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Columns and View a Glimpse of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping a few columns:\n",
    "dropped_columns=[\"overview\",\"belongs_to_collection\",\"production_companies\",\"production_countries\",\"status\",\"original_title\"]\n",
    "df_dropped = df.drop(*dropped_columns)\n",
    "df_dropped.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe with the selected columns:\n",
    "select_column = ['id','budget','popularity','release_date','revenue','title']\n",
    "df = df.select(*select_column)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting Columns (another way based on index)\n",
    "df = spark.read.format(file_type)\\\n",
    ".option(\"inferSchema\", infer_schema)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(file_location)\n",
    "\n",
    "df=df.select(df[2],df[1],df[6],df[9],df[10],df[14])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values (for one test variable (e.g., popularity))\n",
    "from pyspark.sql.functions import *\n",
    "df.filter((df['popularity']=='') | (df['popularity'].isNull()) | (isnan(df['popularity']))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values from all columns:\n",
    "df.select([count(when((col(c)=='') | col(c).isNull()| isnan(c),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way Frequencies\n",
    "df.groupBy(df['title']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequencies - descending order\n",
    "df.groupby(df['title']).count().sort(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: grouby is an alias for groupBy; both the variants will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and Filtering One-way frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  One way frquencies with filters\n",
    "\n",
    "# first creating a temporary dataframe:\n",
    "df_temp = df.filter((df['title']!='')&(df['title'].isNotNull())&(~isnan(df['title'])))\n",
    "\n",
    "# showing the filtered results:\n",
    "df_temp.groupby(df_temp['title']).count().filter(\"`count`>4\").sort(desc(\"count\")).show(10)\n",
    "\n",
    "# counting the number:\n",
    "df_temp.groupby(df_temp['title']).count().filter(\"`count`>4\").sort(desc(\"count\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any temporary dataframe that we created in the process\n",
    "del df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before Casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting\n",
    "df = df.withColumn('budget',df['budget'].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting multiple variables\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Identifying and assiging lists of variables  \n",
    "int_vars=['id']\n",
    "float_vars=['budget', 'popularity', 'revenue']\n",
    "date_vars=['release_date']\n",
    "\n",
    "for column in int_vars:\n",
    "    df=df.withColumn(column,df[column].cast(IntegerType()))\n",
    "for column in float_vars:\n",
    "    df=df.withColumn(column,df[column].cast(FloatType()))\n",
    "for column in date_vars:\n",
    "    df=df.withColumn(column,df[column].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Casting output\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe function\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Three parameters have to be passed through *approxQuantile* function, as follows:\n",
    "\n",
    "* ***col*** - the name of the numerical column\n",
    "* ***probabilities*** - a list of quantile probabilities between [0,1]: 0: minimum; 0.5: median; 1: maximum\n",
    "* ***relativeError*** - The relative target precision to achieve (>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum/Median/Maximum Calculation:\n",
    "df_temp = df.filter((df['budget']!=0)&(df['budget'].isNotNull()) & (~isnan(df['budget'])))\n",
    "\n",
    "# minimum value:\n",
    "min_val=df_temp.approxQuantile('budget',[0.0],0.001)\n",
    "# median value:\n",
    "median=df_temp.approxQuantile('budget',[0.5],0.001)\n",
    "# maximum value:\n",
    "max_val=df_temp.approxQuantile('budget',[1.0],0.001)\n",
    "\n",
    "print (f'The minimum budget is: {min_val} ')\n",
    "print (f'The median of budget is: {median} ')\n",
    "print (f'The maximum budget is: {max_val} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique/Distinct Values and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct Counts\n",
    "df.agg(countDistinct(col(\"title\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Distinct Values\n",
    "df.select('title').distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct Aggregations (time)\n",
    "# First check how the release date column look like:\n",
    "sel_column = ['release_date']\n",
    "df_date = df.select(*sel_column)\n",
    "df_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.withColumn('release_year', year('release_date')) \n",
    "df_temp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.groupBy(\"release_year\").agg(countDistinct(\"title\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime extractions\n",
    "df_temp=df_temp.withColumn('release_month',month('release_date'))\n",
    "df_temp.groupBy(\"release_month\").agg(countDistinct(\"title\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_temp.withColumn('release_day',dayofmonth('release_date'))\n",
    "df_temp.groupBy(\"release_day\").agg(countDistinct(\"title\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of datetime extractions\n",
    "df_temp.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering based on like\n",
    "# title starting with 'Meet' in the following example:\n",
    "df.filter(df['title'].like('Meet%')).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering based on not like\n",
    "# Titles that does not end with 's' in the following example:\n",
    "df.filter(~df['title'].like('%s')).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering based on not regular expressions - method 1\n",
    "df.filter(df['title'].rlike('[A-Z]*ove')).show(5,False)\n",
    "df.filter(df['title'].rlike('[A-Z]*ove')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering based on contain function - method 2\n",
    "df.filter(df.title.contains('ove')).show(5, False)\n",
    "df.filter(df.title.contains('ove')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering based on regular expression - another method\n",
    "# \\w: identifies all upper and lowercase alphabets and numbers from 0 to 9\n",
    "df.filter(df['title'].rlike('\\w*ove')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Columns\n",
    "mean_pop=df.agg({'popularity': 'mean'}).collect()[0]['avg(popularity)']\n",
    "mean_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_obs= df.count()\n",
    "count_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lit function is a way to interact with column literals. It is very useful \n",
    "# when you want to create a column with a value directly\n",
    "df=df.withColumn('mean_popularity',lit(mean_pop))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('varaiance',pow((df['popularity']-df['mean_popularity']),2))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Columns - output\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance calculation:\n",
    "variance_sum = df.agg({'varaiance':'sum'}).collect()[0]['sum(varaiance)']\n",
    "print(f\"Variance Summation: {variance_sum}\")\n",
    "\n",
    "variance_population = variance_sum/(count_obs - 1)\n",
    "print(f\"Variance: {variance_population}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation:\n",
    "import math\n",
    "math.sqrt(variance_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the user defined function on the dataframe:\n",
    "# Step 1. First create the user defined function:\n",
    "def new_cols(budget, popularity):\n",
    "    if budget < 10000000:\n",
    "        budget_cat = 'small'\n",
    "    elif budget < 100000000:\n",
    "        budget_cat = 'medium'\n",
    "    else:\n",
    "        budget_cat = 'big'\n",
    "    \n",
    "    if popularity < 3:\n",
    "        ratings = 'low'\n",
    "    elif popularity < 5:\n",
    "        ratings = 'mid'\n",
    "    else:\n",
    "        ratings = 'high'\n",
    "    return budget_cat, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Field Type of the New Columns\n",
    "udfB=udf(new_cols,StructType([StructField(\"budget_cat\", StringType(), True),StructField(\"ratings\", StringType(), True)]))\n",
    "temp_df=df.select('id','budget','popularity').withColumn(\"newcat\",udfB(\"budget\",\"popularity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Unbundle the struct type columns into individual columns and drop the struct type \n",
    "df_with_newcols = temp_df.select('id','budget','popularity','newcat').withColumn('budget_cat', temp_df.newcat.getItem('budget_cat')).withColumn('ratings', temp_df.newcat.getItem('ratings')).drop('newcat')\n",
    "df_with_newcols.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  New Columns - Observe Metadata\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of creating the columns (with conditions)\n",
    "df_with_newcols = df.select('id','budget','popularity').\\\n",
    "withColumn('budget_cat', when(df['budget']<10000000,'Small').when(df['budget']<100000000,'Medium').otherwise('Big')).\\\n",
    "withColumn('ratings', when(df['popularity']<3,'Low').when(df['popularity']<5,'Mid').otherwise('High'))\n",
    "\n",
    "df_with_newcols.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting and Renaming Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_newcols.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'budget_cat' column:\n",
    "columns_to_drop = ['budget_cat']\n",
    "df_with_newcols = df_with_newcols.drop(*columns_to_drop)\n",
    "\n",
    "df_with_newcols.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming a column:\n",
    "df_with_newcols = df_with_newcols.withColumnRenamed('id','film_id').withColumnRenamed('ratings','film_ratings')\n",
    "\n",
    "df_with_newcols.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to stop spark when you are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
